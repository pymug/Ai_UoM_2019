{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tokenization_and_lemmatization_demonstration",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nam4dev/nlp_demonstration/blob/master/tokenization_and_lemmatization_demonstration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vq6z_QEKZsRg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import spaCy library\n",
        "import spacy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4M_bd7AaFW4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load english model (en_core_web_sm)\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAR2njdDaRU4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_string = \"Hi, I'm giving some attention to the NLP course :)\"\n",
        "\n",
        "# Load the text into a Doc instance\n",
        "doc = nlp(test_string)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3DCmBmpazdy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extracting the tokens\n",
        "tokens = [token.text for token in doc]\n",
        "\n",
        "print(tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9aIo4UPehbs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extracting the Lemmas\n",
        "lemmas = [token.lemma_ for token in doc]\n",
        "\n",
        "print(lemmas)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCozAN03BdqP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import data manipulation library: pandas\n",
        "import pandas as pd\n",
        "# Import tweeter scraping needed libraries\n",
        "import bs4\n",
        "import json\n",
        "import requests"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2_friLHBejI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "response = requests.get('https://twitter.com/tweeter?lang=fr')\n",
        "html_doc = response.text\n",
        "soup = bs4.BeautifulSoup(html_doc, 'html.parser')\n",
        "\n",
        "lines = []\n",
        "for tweet in soup.find_all('p', class_='tweet-text'):\n",
        "  for line in tweet.text.split('\\n'):\n",
        "    stripped = line.strip()\n",
        "    if stripped:\n",
        "      lines.append(stripped)\n",
        "\n",
        "print('Fetched lines', json.dumps(lines, indent=4))\n",
        "print()\n",
        "\n",
        "df = pd.DataFrame(data={\"tweets\": lines})\n",
        "\n",
        "print(df.head())\n",
        "print()\n",
        "\n",
        "print(df.info())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WU2eyW3jBXYk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
        "\n",
        "# Function to preprocess text\n",
        "def preprocess(text):\n",
        "  \t# Create Doc object\n",
        "    doc = nlp(text, disable=['ner', 'parser'])\n",
        "    # Generate lemmas\n",
        "    lemmas = [token.lemma_ for token in doc]\n",
        "    # Remove stopwords and non-alphabetic characters\n",
        "    a_lemmas = [lemma for lemma in lemmas \n",
        "            if lemma.isalpha() and lemma not in stopwords]\n",
        "    \n",
        "    return ' '.join(a_lemmas)\n",
        "  \n",
        "# Apply preprocess to df['transcript']\n",
        "df['transcript'] = df['tweets'].apply(preprocess)\n",
        "print(df['transcript'])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}